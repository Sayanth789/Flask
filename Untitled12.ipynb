{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## BERT End to End (fine-tuning + Predicting) in 5 minutes with Cloud TPU\n",
        "\n",
        "BERT, or Bidirectional Embedding Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.\n",
        "\n",
        " This Colab demonstates using a free Colab Cloud TPU to fine-tune sentence and sentence-pair classification tasks built on top of pretrained BERT models and run predictions on tuned model. The colab demonsrates loading pretrained BERT models from both TF Hub and checkpoints.\n"
      ],
      "metadata": {
        "id": "HaTZRP3mvqrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KWMsa__xMfu",
        "outputId": "d18119d5-d6b9-4190-b155-21c1aa5cecc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (6.33.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.4-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m132.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.4-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorboard-data-server, google_pasta, tensorboard, astunparse, tensorflow\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.9.23 google_pasta-0.2.0 libclang-18.1.1 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 werkzeug-3.1.4 wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "assert 'COLAB TPU ADDR' in os.environ , 'ERROR : Not connected to TPU runtime'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TOU Address is', TPU_ADDRESS)\n",
        "\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print(\"TPU devices: \")\n",
        "  pprint.pprint(sesssion.list_devices)\n",
        "\n",
        "  # Upload credentials to TPU\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "BJj7j_FRwA1L",
        "outputId": "5e30d4b1-14e9-4d94-dd19-32bec8cf863b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ERROR : Not connected to TPU runtime",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3824375608.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m'COLAB TPU ADDR'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'ERROR : Not connected to TPU runtime'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mTPU_ADDRESS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'grpc://'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'COLAB_TPU_ADDR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TOU Address is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTPU_ADDRESS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: ERROR : Not connected to TPU runtime"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OuhGpTLFycwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare and IMport BERT modules\n",
        "\n",
        "With the environement is configured, now we can import the BERT modules. The following step clones t esource code from GitHub ad import the modules from the source. Alternatively we can install BERT using pip"
      ],
      "metadata": {
        "id": "1fKP7plqxGJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "!test -d bert_repo || git clone  https://github.com/google-research/bert bert_repo\n",
        "\n",
        "if not 'bert_repo' in sys.path:\n",
        "  sys.path += ['bert_repo']\n",
        "\n",
        "  # importing the python modules by BERT\n",
        "import modeling\n",
        "import optmization\n",
        "import run_classifier\n",
        "import run_classifier_with_tfhub\n",
        "import tokenization\n",
        "\n",
        "# import tfhub\n",
        "import tensrflow_hub as hub\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_lyxj0IMwAyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing for Training\n",
        "\n",
        "This next section of code performs the following tasks:\n",
        "* Specify task and download the training data\n",
        "* Specify BERT pretrained model\n",
        "* Specify GS bucket , create output directory for model checkpoints and eval results\n",
        "\n"
      ],
      "metadata": {
        "id": "rcV_y4K2y5w2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TASK = 'MRPC' #@param {type:\"string\"}\n",
        "assert TASK in ('MRPC', 'CoLA'), 'Only (MRPC, CoLA) are demonstrated here.'\n",
        "\n",
        "# Download glue data.\n",
        "! test -d download_glue_repo || git clone https://gist.github.com/60c2bdb54d156a41194446737ce03e2e.git download_glue_repo\n",
        "!python download_glue_repo/download_glue_data.py --data_dir='glue_data' --tasks=$TASK\n",
        "\n",
        "TASK_DATA_DIR = 'glue_data/' + TASK\n",
        "print('***** Task data directory: {} *****'.format(TASK_DATA_DIR))\n",
        "!ls $TASK_DATA_DIR\n",
        "\n",
        "BUCKET = 'YOUR_BUCKET' #@param {type:\"string\"}\n",
        "assert BUCKET, 'Must specify an existing GCS bucket name'\n",
        "OUTPUT_DIR = 'gs://{}/bert-tfhub/models/{}'.format(BUCKET, TASK)\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
        "\n",
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "#   cased_L-12_H-768_A-12: cased BERT large model\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_' + BERT_MODEL + '/1'"
      ],
      "metadata": {
        "id": "PeS_a0SKzbGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's load tokenizer module from TF Hub and play with it\n"
      ],
      "metadata": {
        "id": "_8PHoJJY0g_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = run_classifier_with_tfhub.create_tokenizer_from_hub_module(BERT_MODEL_HUB)\n",
        "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
      ],
      "metadata": {
        "id": "-F9eJwLHzny8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also we initialize our hyperparameters, prepare the training data and initialize TPU config\n"
      ],
      "metadata": {
        "id": "K-zaoXWh0sNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 8\n",
        "PREDICT_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 3.0\n",
        "NUM_TRAIN_EPOCHS  = 3.0\n",
        "MAX_SEQ_LENGTH = 128\n",
        "\n",
        "# Warmup is a period of time where the learning rate is small and gradually increses--ususally\n",
        "# helps training.\n",
        "\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 1000\n",
        "SAVE_SUMMARY_STEPS = 500\n",
        "\n",
        "\n",
        "processors = {\n",
        "    'cola': run_classifier.ColaProcessor,\n",
        "    'mnli': run_classifier.MnliProcessor,\n",
        "    'mrpc': run_classifier.MrpcProcessor,\n",
        "}\n",
        "\n",
        "processor = processors[TASK.lower()]()\n",
        "label_list = processor.get_labels()\n",
        "\n",
        "\n",
        "# Compute the number of train and warmups from batch_size\n",
        "train_examples = processor.get_train_examples[TASK_DATA_DIR]\n",
        "num_train_steps = int(len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmups_steps =  int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "# Setup TPU related config\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "NUM_TPU_CORES = 8\n",
        "ITERATIONS_PER_LOOP = 1000\n",
        "\n",
        "def get_run_config(output_dir):\n",
        "  return tf.contrib.tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      model_dir=output_dir,\n",
        "      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "          iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "          num_shards=NUM_TPU_CORES,\n",
        "          per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2)\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r6hmpC8401Vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine_tune and RUN Predictions on a pretrained BER model from TF Hub\n",
        "This section demonstrates the fine-tuning from pre-trained BERT TF Hub module and running predictions.\n"
      ],
      "metadata": {
        "id": "pHBlOd41294Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TFHUB_CACHE_DIR\"] = OUTPUT_DIR\n",
        "\n",
        "model_fn = run_classifier_with_tfhub.model_fn_builder(\n",
        "    num_labels=len(label_list),\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmups_steps,\n",
        "    use_tpu=True,\n",
        "    bert_hub_module=handle=BERT_MODEL_HUB\n",
        "\n",
        ")\n",
        "\n",
        "estimator_from_fthub = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu = True,\n",
        "    model_fn=model_fn,\n",
        "    config=get_config(OUTPUT_DIR),\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE\n",
        "    predict_batch_size=PREDICT_BATCH_SIZE,\n",
        ")"
      ],
      "metadata": {
        "id": "iphSqKRm3Quq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we can fine-tune the model, evaluate it  and run predictions on it.\n"
      ],
      "metadata": {
        "id": "K5TLJQR74GHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "def model_train(estimator):\n",
        "  print('MRPC/CoLA on BERT base model normally takes about 2-3 minutes. Please wait...')\n",
        "  # We'll set sequences to be at most 128 tokens long.\n",
        "  train_features = run_classifier.convert_examples_to_features(\n",
        "      train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(train_examples)))\n",
        "  print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
        "  tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "  train_input_fn = run_classifier.input_fn_builder(\n",
        "      features=train_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=True,\n",
        "      drop_remainder=True)\n",
        "  estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "  print('***** Finished training at {} *****'.format(datetime.datetime.now()))\n",
        "\n"
      ],
      "metadata": {
        "id": "rz9D7JgR4Mrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_train(estimator_from_tfhub)"
      ],
      "metadata": {
        "id": "KD2KztwD5buc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_eval(estimator):\n",
        "  # Eval the model\n",
        "  eval_examples = processor.get_dev_examples(TASK_DATA_DIR)\n",
        "  eval_fetures = run_classifier.convert_examples_to_features(\n",
        "      eval_examples, label_list, MAX_SEQ_LENGTH, tokenizer\n",
        "  )\n",
        "\n",
        "  print(\"**** Started evaluation at {} ****\".format(datetime.datetime.now()))\n",
        "  print(\"Num examles = {}\".format(len(eval_examples)))\n",
        "  print(\" Batch size = {} \".format(model_train(estimator_from_fthub)))\n",
        "\n",
        "\n",
        "  # eval will be slghtly WRONG on te TPU because it will truncate  the last batch\n",
        "  eval_steps = int(len(eval_examples) / EVAL_BATCH_SIZE)\n",
        "  eval_input_fn = run_classifier.input_fn_builder(\n",
        "      features=eval_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=True)\n",
        "\n",
        "  result = estimator.evaluate(input_fn=input_fn, steps=eval_steps)\n",
        "  print('**** Finished evaluation at {} ****'.format(datetime.datetime.now()))\n",
        "\n",
        "  output_eval_file = os.path.join(OUTPUT_DIR, \"eval_results.txt\")\n",
        "  with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "    print(\"*** Eval Results ***\")\n",
        "    for key in sorted(result.keys()):\n",
        "      print(' {} = {} '.format(key, str(result[key])))\n",
        "      writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
      ],
      "metadata": {
        "id": "OrP5J3FC5iUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_eval(estimator_from_fthub)"
      ],
      "metadata": {
        "id": "kuTvA5NJ7CzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_predict(estimator):\n",
        "  # Make prediction on a subset of eval examples\n",
        "  prediction_examples = processor.get_dev_examples(TASK_DATA_DIR)[:PREDICT_BATCH_SIZE]\n",
        "  input_features = run_classifier.convert_examples_to_features(\n",
        "      prediction_examples, label_list, MAX_SEQ_LENGTH, tokenizer\n",
        "  )\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features,seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n",
        "  predictions = estimator.predict(predict_input_fn)\n",
        "\n",
        "  for example, prediction in zip(prediction_examples, predictions):\n",
        "    print('text_a: %s\\ntext_b: %s\\nlabel:%s\\nprediction:%s\\n' % (example.text_a, example.text_b, str(example.label), prediction['probabilities']))\n",
        "\n"
      ],
      "metadata": {
        "id": "tGYufelm7O3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_predict(estimator_from_fthub)"
      ],
      "metadata": {
        "id": "ovAV5JZ-8PIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune and run predictions on a pre-trained BERT model from checkpoints\n"
      ],
      "metadata": {
        "id": "wx1RBNHb8Vy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup task specific model and TPU running config.\n",
        "\n",
        "BERT_PRETRAINED_DIR = 'gs://cloud-tpu-checkpoints/bert/' + BERT_MODEL\n",
        "print(\"*** BERT pretrained directory: {} ****\".format(BERT_PRETRAINED_DIR))\n",
        "!gsutil ls $BERT_PRETRAINED_DIR\n",
        "\n",
        "CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, \"bert_config.json\")\n",
        "INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, \"bert_model.ckpt\")\n",
        "\n",
        "\n",
        "model_fn = run_classifier.model_fn_builder(\n",
        "    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "    num_labels = len(label_list),\n",
        "    init_checkpoint=INIT_CHECKPOINT,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmups_steps,\n",
        "    use_tpu=True,\n",
        "    use_one_hot_embeddings=True\n",
        ")\n",
        "\n",
        "OUTPUT_DIR = OUTPUT_DIR.replace('bert-tfhub', 'bert-checkpoints')\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "\n",
        "estimator_from_checkpoints = tf.config.tpu.TPUEstimator(\n",
        "    ues_tpus=True,\n",
        "    model_fn=model_fn,\n",
        "    config=get_run_config(OUTPUT_DIR),\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE,,\n",
        "    predict_batch_size=PREDICT_BATCH_SIZE,\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4yEsavhe8XKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_train(estimator_from_checkpoints)"
      ],
      "metadata": {
        "id": "Yw5p7E638d-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_eval(estimator_from_checkpoints)"
      ],
      "metadata": {
        "id": "rlGJS4179vTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_predict(estimator_from_checkpoints)"
      ],
      "metadata": {
        "id": "vhutRdvg9yTF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}